{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dafa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "from utils.eval_metrics import evaluate_performance_metric\n",
    "from utils.load_task import load_test_split, format_data\n",
    "from utils.response_standardization import cut_double_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ad92e",
   "metadata": {},
   "source": [
    "# PAWS-X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb470bd",
   "metadata": {},
   "source": [
    "### en - paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"paraphrases/task/individual_paws-x/temp-0.2_topp-1.0_maxt-2048/combined_True/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "    datapoint = datapoint.replace(\"Paraphrased sentence\", \"Sentence\").replace(\"1. \", \"\").replace(\"2. \", \"\")\n",
    "    if len(datapoint.split(\"\\n\")) == 2:\n",
    "        s1, s2 = datapoint.split(\"\\n\")\n",
    "        if not \"sentence 1\" in s1.lower():\n",
    "            s1 = \"Sentence 1: \" + s1\n",
    "        if not \"sentence 2\" in s2.lower():\n",
    "            s2 = \"Sentence 2: \" + s2\n",
    "    else:\n",
    "        print(\"NOT SOLVED\")\n",
    "    sentence1 = s1.split(\"Sentence 1\")[1].strip(\" :\")\n",
    "    sentence2 = s2.split(\"Sentence 2\")[1].strip(\" :\")\n",
    "    \n",
    "    if sentence1 != \"\":\n",
    "        sentence1 = cut_double_quotes(sentence1)\n",
    "    if sentence2 != \"\":\n",
    "        sentence2 = cut_double_quotes(sentence2)\n",
    "    \n",
    "    sentences[\"sentence1\"].append(sentence1)\n",
    "    sentences[\"sentence2\"].append(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)\n",
    "with open(path + \"dataset_orig.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_orig, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef32a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"id\": [], \"label\": [], \"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for key in [\"id\", \"label\", \"sentence1\", \"sentence2\"]:\n",
    "    for i in range(len(sentences[\"sentence1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "            \n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)\n",
    "    \n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99bbce",
   "metadata": {},
   "source": [
    "### de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_paws-x/temp-0.2_topp-1.0_maxt-2048/combined_True/de_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT GERMAN\n",
    "\n",
    "sentences = {\"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for datapoint in data_orig[\"content\"]:\n",
    "    s1, s2 = datapoint.split(\"\\n\")\n",
    "    sentence1 = s1.split(\"Satz 1\")[1].strip(\" :\")\n",
    "    sentence2 = s2.split(\"Satz 2\")[1].strip(\" :\")\n",
    "    sentences[\"sentence1\"].append(cut_double_quotes(sentence1))\n",
    "    sentences[\"sentence2\"].append(cut_double_quotes(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)\n",
    "with open(path + \"dataset_orig.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_orig, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad40127",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"id\": [], \"label\": [], \"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for key in [\"id\", \"label\", \"sentence1\", \"sentence2\"]:\n",
    "    for i in range(len(sentences[\"sentence1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bceb1ec",
   "metadata": {},
   "source": [
    "### nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_paws-x/temp-0.2_topp-1.0_maxt-2048/combined_True/nl_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de936506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT NL\n",
    "\n",
    "sentences = {\"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    if len(datapoint.split(\"\\n\")) < 2:\n",
    "        sentence1 = datapoint\n",
    "        sentence2 = \"\"\n",
    "    elif len(datapoint.split(\"\\n\")) == 2:\n",
    "        sentence1, sentence2 = datapoint.split(\"\\n\")\n",
    "        \n",
    "    if not \"Zin \" in sentence1:\n",
    "            sentence1 = \"Zin 1: \" + \"\\\"\" + sentence1 + \"\\\"\"\n",
    "    if not \"Zin \" in sentence2:\n",
    "            sentence2 = \"Zin 2: \" + \"\\\"\" + sentence2 + \"\\\"\"\n",
    "    \n",
    "    s1 = sentence1.split(\"Zin 1\")[1].strip(\" :\")\n",
    "    s2 = sentence2.split(\"Zin 2\")[1].strip(\" :\")\n",
    "    \n",
    "    if s1.strip(\"\\\"\") != \"\": \n",
    "        s1 = cut_double_quotes(s1)\n",
    "    if s2.strip(\"\\\"\") != \"\": \n",
    "        s2 = cut_double_quotes(s2)\n",
    "    \n",
    "    sentences[\"sentence1\"].append(s1)\n",
    "    sentences[\"sentence2\"].append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"id\": [], \"label\": [], \"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for key in [\"id\", \"label\", \"sentence1\", \"sentence2\"]:\n",
    "    for i in range(len(sentences[\"sentence1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9174d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb31b85",
   "metadata": {},
   "source": [
    "### sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_paws-x/temp-0.2_topp-1.0_maxt-2048/combined_True/sv_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292eab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT SV\n",
    "\n",
    "sentences = {\"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    if len(datapoint.split(\"\\n\")) < 2:\n",
    "        sentence1 = datapoint\n",
    "        sentence2 = \"\"\n",
    "    elif len(datapoint.split(\"\\n\")) == 2:\n",
    "        sentence1, sentence2 = datapoint.split(\"\\n\")\n",
    "        \n",
    "    if \"Sats\" in sentence1:\n",
    "        sentence1 = sentence1.replace(\"Sats\", \"Mening\")\n",
    "    if \"Sats\" in sentence2:\n",
    "        sentence2 = sentence2.replace(\"Sats\", \"Mening\")\n",
    "    if \"Sentence\" in sentence1:\n",
    "        sentence1 = sentence1.replace(\"Sentence\", \"Mening\")\n",
    "    if \"Sentence\" in sentence2:\n",
    "        sentence2 = sentence2.replace(\"Sentence\", \"Mening\")\n",
    "        \n",
    "    if not \"Mening \" in sentence1:\n",
    "            sentence1 = \"Mening 1: \" + \"\\\"\" + sentence1 + \"\\\"\"\n",
    "    if not \"Mening \" in sentence2:\n",
    "            sentence2 = \"Mening 2: \" + \"\\\"\" + sentence2 + \"\\\"\"\n",
    "            \n",
    "    s1 = sentence1.split(\"Mening 1\")[1].strip(\" :\")\n",
    "    s2 = sentence2.split(\"Mening 2\")[1].strip(\" :\")\n",
    "        \n",
    "    sentences[\"sentence1\"].append(cut_double_quotes(s1))\n",
    "    sentences[\"sentence2\"].append(cut_double_quotes(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be65cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"id\": [], \"label\": [], \"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for key in [\"id\", \"label\", \"sentence1\", \"sentence2\"]:\n",
    "    for i in range(len(sentences[\"sentence1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "\n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4da151",
   "metadata": {},
   "source": [
    "### it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_paws-x/temp-0.2_topp-1.0_maxt-2048/combined_True/it_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT IT\n",
    "\n",
    "sentences = {\"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    if \"\\n\\n\" in datapoint:\n",
    "        datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "    if len(datapoint.split(\"\\n\")) < 2:\n",
    "        print(i, \"TOO SHORT\", datapoint)\n",
    "        sentence1 = datapoint\n",
    "        sentence2 = \"\"\n",
    "    elif len(datapoint.split(\"\\n\")) == 2:\n",
    "        sentence1, sentence2 = datapoint.split(\"\\n\")\n",
    "    \n",
    "    if \"Sentence 1\" in sentence1:\n",
    "        sentence1 = sentence1.replace(\"Sentence 1\", \"Frase 1\")\n",
    "    if \"Sentence 2\" in sentence2:\n",
    "        sentence2 = sentence2.replace(\"Sentence 2\", \"Frase 2\")\n",
    "    if not \"Frase \" in sentence1:\n",
    "            sentence1 = \"Frase 1: \" + \"\\\"\" + sentence1 + \"\\\"\"\n",
    "    if not \"Frase \" in sentence2:\n",
    "            sentence2 = \"Frase 2: \" + \"\\\"\" + sentence2 + \"\\\"\"\n",
    "            \n",
    "    s1 = sentence1.split(\"Frase 1\")[1].strip(\" :\")\n",
    "    s2 = sentence2.split(\"Frase 2\")[1].strip(\" :\")\n",
    "    \n",
    "    if s1.strip(\"\\\"\") != \"\": \n",
    "        s1 = cut_double_quotes(s1)\n",
    "    if s2.strip(\"\\\"\") != \"\": \n",
    "        s2 = cut_double_quotes(s2)\n",
    "        \n",
    "    sentences[\"sentence1\"].append(s1)\n",
    "    sentences[\"sentence2\"].append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"id\": [], \"label\": [], \"sentence1\": [], \"sentence2\": []}\n",
    "\n",
    "for key in [\"id\", \"label\", \"sentence1\", \"sentence2\"]:\n",
    "    for i in range(len(sentences[\"sentence1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5eb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f9a95",
   "metadata": {},
   "source": [
    "# XNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c0316",
   "metadata": {},
   "source": [
    "### en - paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"paraphrases/task/xglue_xnli/temp-0.2_topp-1.0_maxt-2048/combined_True/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "    datapoint = datapoint.replace(\"Paraphrased premise\", \"Premise\").replace(\"Paraphrased hypothesis\", \"Hypothesis\")\n",
    "    if len(datapoint.split(\"\\n\")) == 4:\n",
    "        datapoint = datapoint.split(\"\\n\")[1] + \"\\n\" + datapoint.split(\"\\n\")[3]\n",
    "    if len(datapoint.split(\"\\n\")) == 2:\n",
    "        s1, s2 = datapoint.split(\"\\n\")\n",
    "        if not \"premise\" in s1.lower():\n",
    "            s1 = \"Premise: \" + s1\n",
    "        if not \"hypothesis\" in s2.lower():\n",
    "            s2 = \"Hypothesis: \" + s2              \n",
    "    else:\n",
    "        print(i, \"NOT SOLVED\", \"\\n\", datapoint, \"\\n\")\n",
    "    sentence1 = s1.split(\"Premise\")[1].strip(\" :\")\n",
    "    sentence2 = s2.split(\"Hypothesis\")[1].strip(\" :\")\n",
    "    \n",
    "    if sentence1 != \"\":\n",
    "        sentence1 = cut_double_quotes(sentence1)\n",
    "    if sentence2 != \"\":\n",
    "        sentence2 = cut_double_quotes(sentence2)\n",
    "    \n",
    "    sentences[\"premise\"].append(sentence1)\n",
    "    sentences[\"hypothesis\"].append(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)\n",
    "    \n",
    "data_combined = {\"label\": [], \"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for key in [\"label\", \"premise\", \"hypothesis\"]:\n",
    "    for i in range(len(sentences[\"premise\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "\n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf337443",
   "metadata": {},
   "source": [
    "### de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/xglue_xnli/temp-0.2_topp-1.0_maxt-2048/combined_True/de_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize XNLI content german\n",
    "\n",
    "sentences = {\"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    flag = False\n",
    "    datapoint = datapoint.split(\"\\n\")\n",
    "    datapoint = [d for d in datapoint if d!=\"\"]\n",
    "    \n",
    "    if len(datapoint) != 2:\n",
    "        print(\"ORIG\", datapoint)\n",
    "        flag = True\n",
    "    \n",
    "    if len(datapoint) == 4:\n",
    "        datapoint = [datapoint[1]] + [datapoint[3]]\n",
    "    if len(datapoint) == 3:\n",
    "        datapoint = datapoint[1:]\n",
    "    if len(datapoint) == 5:\n",
    "        datapoint = [datapoint[2]] + [datapoint[4]]\n",
    "    if len(datapoint) == 1:\n",
    "        datapoint = datapoint + [\"\"]\n",
    "    \n",
    "    if len(datapoint) == 2:\n",
    "        sentence1 = datapoint[0]\n",
    "        sentence2 = datapoint[1]\n",
    "    \n",
    "        if \"Ausgangspunkt\" in sentence1: \n",
    "            sentence1 = sentence1.replace(\"Ausgangspunkt\", \"Prämisse\")\n",
    "        if \"Voraussetzung\" in sentence1:\n",
    "            sentence1 = sentence1.replace(\"Voraussetzung\", \"Prämisse\")\n",
    "        if \"Ausgangssituation\" in sentence1:\n",
    "            sentence1 = sentence1.replace(\"Ausgangssituation\", \"Prämisse\")\n",
    "        if \"Premise\" in sentence1:\n",
    "            sentence1 = sentence1.replace(\"Premise\", \"Prämisse\")\n",
    "        if \"German translation\" in sentence1:\n",
    "            sentence1 = sentence1.replace(\"German translation\", \"Prämisse\")\n",
    "        if \"Hypothesis\" in sentence2:\n",
    "            sentence2 = sentence2.replace(\"Hypothesis\", \"Hypothese\")\n",
    "        if \"German translation\" in sentence2:\n",
    "            sentence2 = sentence2.replace(\"German translation\", \"Hypothese\")\n",
    "        if not \"Prämisse\" in sentence1:\n",
    "            sentence1 = \"Prämisse: \" + sentence1\n",
    "        if not \"Hypothese\" in sentence2:\n",
    "            sentence2 = \"Hypothese: \" + sentence2\n",
    "        \n",
    "        s1 = sentence1.split(\"Prämisse\")[1].strip(\": \")\n",
    "        s2 = sentence2.split(\"Hypothese\")[1].strip(\": \")\n",
    "        \n",
    "        if s1 != \"\":\n",
    "            s1 = cut_double_quotes(s1)\n",
    "        if s2 != \"\":\n",
    "            s2 = cut_double_quotes(s2)\n",
    "            \n",
    "        sentences[\"premise\"].append(s1)\n",
    "        sentences[\"hypothesis\"].append(s2)\n",
    "    \n",
    "        if flag:\n",
    "            print(\"FILTERED\", sentence1, sentence2)\n",
    "    \n",
    "    else:\n",
    "        print(len(datapoint))\n",
    "        print(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for key in [\"label\", \"premise\", \"hypothesis\"]:\n",
    "    for i in range(len(sentences[\"premise\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd644881",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6607a",
   "metadata": {},
   "source": [
    "### sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/xglue_xnli/temp-0.2_topp-1.0_maxt-2048/combined_True/sv_from_en/\"\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for i in range(len(data_orig[\"content\"])):\n",
    "    datapoint = data_orig[\"content\"][i]\n",
    "    if \"\\n\\n\" in datapoint:\n",
    "        datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "    if \"Förutsättning\" in datapoint: \n",
    "        datapoint = datapoint.replace(\"Förutsättning\", \"Premiss\")\n",
    "    if len(datapoint.split(\"\\n\")) == 2:\n",
    "        sentence1, sentence2 = datapoint.split(\"\\n\")\n",
    "        \n",
    "        if not \"Premiss\" in sentence1:\n",
    "            sentence1 = \"Premiss: \" + sentence1\n",
    "        if not \"Hypotes\" in sentence2:\n",
    "            sentence2 = \"Hypotes: \" + sentence2\n",
    "        \n",
    "        s1 = cut_double_quotes(sentence1.split(\"Premiss: \")[1])\n",
    "        s2 = cut_double_quotes(sentence2.split(\"Hypotes: \")[1])\n",
    "        \n",
    "        sentences[\"premise\"].append(s1)\n",
    "        sentences[\"hypothesis\"].append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for key in [\"label\", \"premise\", \"hypothesis\"]:\n",
    "    for i in range(len(sentences[\"premise\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c0dbf",
   "metadata": {},
   "source": [
    "### nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/xglue_xnli/temp-0.2_topp-1.0_maxt-2048/combined_True/nl_from_en/\"\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for i in range(len(data_orig[\"content\"])):\n",
    "    datapoint = data_orig[\"content\"][i]\n",
    "    if \"\\n\\n\" in datapoint:\n",
    "        datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "    if len(datapoint.split(\"\\n\")) == 2:\n",
    "        sentence1, sentence2 = datapoint.split(\"\\n\")\n",
    "        if \"Uitgangspunt\" in sentence1:\n",
    "            sentence1 = sentence1.replace(\"Uitgangspunt\", \"Premisse\")\n",
    "        if \"Uitnodiging\" in sentence1:\n",
    "            sentence1 = sentence1.replace(\"Uitnodiging\", \"Premisse\")\n",
    "        if not \"Premisse\" in sentence1 or not \"Hypothese\" in sentence2:\n",
    "            print(i, datapoint)\n",
    "        \n",
    "        s1 = cut_double_quotes(sentence1.split(\"Premisse: \")[1])\n",
    "        s2 = cut_double_quotes(sentence2.split(\"Hypothese: \")[1])\n",
    "        \n",
    "        sentences[\"premise\"].append(s1)\n",
    "        sentences[\"hypothesis\"].append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c03fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for key in [\"label\", \"premise\", \"hypothesis\"]:\n",
    "    for i in range(len(sentences[\"premise\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "            \n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d645c0",
   "metadata": {},
   "source": [
    "### it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/xglue_xnli/temp-0.2_topp-1.0_maxt-2048/combined_True/it_from_en/\"\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for i, datapoint in range(len(data_orig[\"content\"])):\n",
    "    if \"\\n\\n\" in datapoint:\n",
    "        datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "    if len(datapoint.split(\"\\n\")) == 2:\n",
    "        sentence1, sentence2 = datapoint.split(\"\\n\")\n",
    "        if \"Premessa\" in sentence1:\n",
    "            sentence1 = sentence1.replace(\"Premessa\", \"Presupposto\")\n",
    "        if not \"Presupposto\" in sentence1 or not \"Ipotesi\" in sentence2:\n",
    "            print(i, datapoint)\n",
    "    else:\n",
    "        print(datapoint)\n",
    "        if \"Premessa\" in datapoint and not \"Ipotesi\" in datapoint:\n",
    "            sentence1 = datapoint\n",
    "            sentence1 = sentence1.replace(\"Premessa\", \"Presupposto\")\n",
    "            sentence2 = \"\"\n",
    "            print(\"S1\", sentence1)\n",
    "            print(\"S2\", sentence2)\n",
    "    if len(sentence1) > 0:\n",
    "        s1 = cut_double_quotes(sentence1.split(\"Presupposto: \")[1])\n",
    "    if len(sentence2) > 0:\n",
    "        s2 = cut_double_quotes(sentence2.split(\"Ipotesi: \")[1])\n",
    "    \n",
    "    sentences[\"premise\"].append(s1)\n",
    "    sentences[\"hypothesis\"].append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"premise\": [], \"hypothesis\": []}\n",
    "\n",
    "for key in [\"label\", \"premise\", \"hypothesis\"]:\n",
    "    for i in range(len(sentences[\"premise\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "            \n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d2997",
   "metadata": {},
   "source": [
    "# COPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea6003",
   "metadata": {},
   "source": [
    "### en - paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a527e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"paraphrases/task/superglue_copa/temp-0.2_topp-1.0_maxt-2048/combined_True/\"\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\"premise\": [], \"choice1\": [], \"choice2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "    datapoint = datapoint.replace(\"Paraphrased premise\", \"Premise\").replace(\"Paraphrased alternative\", \"Alternative\")\n",
    "    if len(datapoint.split(\"\\n\")) == 3:\n",
    "        s1, s2, s3 = datapoint.split(\"\\n\")\n",
    "        if not \"premise\" in s1.lower():\n",
    "            print(i, datapoint)\n",
    "        if not \"alternative\" in s2.lower():\n",
    "            print(i, datapoint)\n",
    "        if not \"alternative\" in s3.lower():\n",
    "            print(i, datapoint)\n",
    "    else:\n",
    "        print(i, \"NOT SOLVED\", \"\\n\", datapoint, \"\\n\")\n",
    "    sentence1 = s1.split(\"Premise\")[1].strip(\" :\")\n",
    "    sentence2 = s2.split(\"Alternative 1\")[1].strip(\" :\")\n",
    "    sentence3 = s3.split(\"Alternative 2\")[1].strip(\" :\")\n",
    "    \n",
    "    sentence1 = cut_double_quotes(sentence1)\n",
    "    sentence2 = cut_double_quotes(sentence2)\n",
    "    sentence3 = cut_double_quotes(sentence3)\n",
    "    \n",
    "    sentences[\"premise\"].append(sentence1)\n",
    "    sentences[\"choice1\"].append(sentence2)\n",
    "    sentences[\"choice2\"].append(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"dataset_orig.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_orig, f)\n",
    "\n",
    "with open(path + \"sentence_translations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)\n",
    "\n",
    "data_combined = {\"label\": [], \"question\": [], \"premise\": [], \"choice1\": [], \"choice2\": [], \"idx\": []}\n",
    "\n",
    "for key in [\"label\", \"question\", \"premise\", \"choice1\", \"choice2\", \"idx\"]:\n",
    "    for i in range(len(sentences[\"choice1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "            \n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a39baa",
   "metadata": {},
   "source": [
    "### de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/superglue_copa/temp-0.2_topp-1.0_maxt-2048/combined_True/de_from_en/\"\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f04c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT DE\n",
    "\n",
    "alternative_names = [\"Ausgangspunkt\", \"Annahme\", \"Ausgangssituation\", \"Voraussetzung\", \"Ausgangslage\"]\n",
    "\n",
    "sentences = {\"premise\": [], \"choice1\": [], \"choice2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 3:\n",
    "        print(\"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint)\n",
    "        \n",
    "    elif len(datapoint.split(\"\\n\")) == 3:\n",
    "        \n",
    "        if \"\\n\\n\" in datapoint:\n",
    "            datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "            \n",
    "        for elem in alternative_names:\n",
    "            if elem in datapoint:\n",
    "                datapoint = datapoint.replace(elem, \"Prämisse\")\n",
    "                \n",
    "        sentence1, sentence2, sentence3 = datapoint.split(\"\\n\")\n",
    "        \n",
    "        if not \"Prämisse\" in sentence1:\n",
    "            print(\"NO PREMISE\", sentence1)\n",
    "        if not \"Alternative 1\" in sentence2:\n",
    "            print(\"NO ALTERNATIVE 1\", sentence2)\n",
    "        if not \"Alternative 2\" in sentence3:\n",
    "            print(\"NO ALTERNATIVE 2\", sentence3)\n",
    "            \n",
    "    if not \"Prämisse\" in sentence1:\n",
    "            sentence1 = \"Prämisse: \" + \"\\\"\" + sentence1 + \"\\\"\"\n",
    "            print(\"no premise\", sentence1)\n",
    "    if not \"Alternative 1\" in sentence2:\n",
    "            sentence2 = \"Alternative 1: \" + \"\\\"\" + sentence2 + \"\\\"\"\n",
    "            print(\"no alternative 1\", sentence2)\n",
    "    if not \"Alternative 2\" in sentence3:\n",
    "            sentence3 = \"Alternative 2: \"+ \"\\\"\" + sentence3 + \"\\\"\"\n",
    "            print(\"no alternative 2\", sentence3)\n",
    "    \n",
    "    s1 = sentence1.split(\"Prämisse\")[1].strip(\" :\")\n",
    "    s2 = sentence2.split(\"Alternative 1\")[1].strip(\" :\")\n",
    "    s3 = sentence3.split(\"Alternative 2\")[1].strip(\" :\")\n",
    "    \n",
    "    if s1.strip(\"\\\"\") != \"\": \n",
    "        s1 = cut_double_quotes(s1)\n",
    "    if s2.strip(\"\\\"\") != \"\": \n",
    "        s2 = cut_double_quotes(s2)\n",
    "    if s3.strip(\"\\\"\") != \"\": \n",
    "        s3 = cut_double_quotes(s3)\n",
    "    \n",
    "    sentences[\"premise\"].append(s1)\n",
    "    sentences[\"choice1\"].append(s2)\n",
    "    sentences[\"choice2\"].append(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f737b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"question\": [], \"premise\": [], \"choice1\": [], \"choice2\": [], \"idx\": []}\n",
    "\n",
    "for key in [\"label\", \"question\", \"premise\", \"choice1\", \"choice2\", \"idx\"]:\n",
    "    for i in range(len(sentences[\"choice1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ede40",
   "metadata": {},
   "source": [
    "### nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/superglue_copa/temp-0.2_topp-1.0_maxt-2048/combined_True/nl_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1363e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT DE\n",
    "\n",
    "alternative_names = [\"Uitgangspunt\"]\n",
    "sentences = {\"premise\": [], \"choice1\": [], \"choice2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 3:\n",
    "        print(\"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint)\n",
    "        \n",
    "    elif len(datapoint.split(\"\\n\")) == 3:\n",
    "        \n",
    "        if \"\\n\\n\" in datapoint:\n",
    "            datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "            \n",
    "        for elem in alternative_names:\n",
    "            if elem in datapoint:\n",
    "                datapoint = datapoint.replace(elem, \"Premisse\")\n",
    "                \n",
    "        sentence1, sentence2, sentence3 = datapoint.split(\"\\n\")\n",
    "        \n",
    "        if not \"Premisse\" in sentence1:\n",
    "            print(\"NO PREMISE\", sentence1)\n",
    "        if not \"Alternatief 1\" in sentence2:\n",
    "            print(\"Alternatief 1\", sentence2)\n",
    "        if not \"Alternatief 2\" in sentence3:\n",
    "            print(\"NO ALTERNATIVE 2\", sentence3)\n",
    "            \n",
    "    if not \"Premisse\" in sentence1:\n",
    "            sentence1 = \"Premisse: \" + \"\\\"\" + sentence1 + \"\\\"\"\n",
    "            print(\"no premise\", sentence1)\n",
    "    if not \"Alternatief 1\" in sentence2:\n",
    "            sentence2 = \"Alternatief 1: \" + \"\\\"\" + sentence2 + \"\\\"\"\n",
    "            print(\"no alternative 1\", sentence2)\n",
    "    if not \"Alternatief 2\" in sentence3:\n",
    "            sentence3 = \"Alternatief 2: \"+ \"\\\"\" + sentence3 + \"\\\"\"\n",
    "            print(\"no alternative 2\", sentence3)\n",
    "    \n",
    "    s1 = sentence1.split(\"Premisse\")[1].strip(\" :\")\n",
    "    s2 = sentence2.split(\"Alternatief 1\")[1].strip(\" :\")\n",
    "    s3 = sentence3.split(\"Alternatief 2\")[1].strip(\" :\")\n",
    "    \n",
    "    if s1.strip(\"\\\"\") != \"\": \n",
    "        s1 = cut_double_quotes(s1)\n",
    "    if s2.strip(\"\\\"\") != \"\": \n",
    "        s2 = cut_double_quotes(s2)\n",
    "    if s3.strip(\"\\\"\") != \"\": \n",
    "        s3 = cut_double_quotes(s3)\n",
    "    \n",
    "    sentences[\"premise\"].append(s1)\n",
    "    sentences[\"choice1\"].append(s2)\n",
    "    sentences[\"choice2\"].append(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"question\": [], \"premise\": [], \"choice1\": [], \"choice2\": [], \"idx\": []}\n",
    "\n",
    "for key in [\"label\", \"question\", \"premise\", \"choice1\", \"choice2\", \"idx\"]:\n",
    "    for i in range(len(sentences[\"choice1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2de40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379fa3d",
   "metadata": {},
   "source": [
    "### it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/superglue_copa/temp-0.2_topp-1.0_maxt-2048/combined_True/it_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT DE\n",
    "\n",
    "alternative_names = [\"Premessa\"]\n",
    "sentences = {\"premise\": [], \"choice1\": [], \"choice2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 3:\n",
    "        print(\"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint)\n",
    "        \n",
    "    elif len(datapoint.split(\"\\n\")) == 3:\n",
    "        \n",
    "        if \"\\n\\n\" in datapoint:\n",
    "            datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "            \n",
    "        for elem in alternative_names:\n",
    "            if elem in datapoint:\n",
    "                datapoint = datapoint.replace(elem, \"Presupposto\")\n",
    "                \n",
    "        sentence1, sentence2, sentence3 = datapoint.split(\"\\n\")\n",
    "        \n",
    "        if not \"Presupposto\" in sentence1:\n",
    "            print(\"NO PREMISE\", sentence1)\n",
    "        if not \"Alternativa 1\" in sentence2:\n",
    "            print(\"Alternativa 1\", sentence2)\n",
    "        if not \"Alternativa 2\" in sentence3:\n",
    "            print(\"NO ALTERNATIVE 2\", sentence3)\n",
    "            \n",
    "    if not \"Presupposto\" in sentence1:\n",
    "            sentence1 = \"Presupposto: \" + \"\\\"\" + sentence1 + \"\\\"\"\n",
    "            print(\"no premise\", sentence1)\n",
    "    if not \"Alternativa 1\" in sentence2:\n",
    "            sentence2 = \"Alternativa 1: \" + \"\\\"\" + sentence2 + \"\\\"\"\n",
    "            print(\"no alternative 1\", sentence2)\n",
    "    if not \"Alternativa 2\" in sentence3:\n",
    "            sentence3 = \"Alternativa 2: \"+ \"\\\"\" + sentence3 + \"\\\"\"\n",
    "            print(\"no alternative 2\", sentence3)\n",
    "    \n",
    "    s1 = sentence1.split(\"Presupposto\")[1].strip(\" :\")\n",
    "    s2 = sentence2.split(\"Alternativa 1\")[1].strip(\" :\")\n",
    "    s3 = sentence3.split(\"Alternativa 2\")[1].strip(\" :\")\n",
    "    \n",
    "    if s1.strip(\"\\\"\") != \"\": \n",
    "        s1 = cut_double_quotes(s1)\n",
    "    if s2.strip(\"\\\"\") != \"\": \n",
    "        s2 = cut_double_quotes(s2)\n",
    "    if s3.strip(\"\\\"\") != \"\": \n",
    "        s3 = cut_double_quotes(s3)\n",
    "    \n",
    "    sentences[\"premise\"].append(s1)\n",
    "    sentences[\"choice1\"].append(s2)\n",
    "    sentences[\"choice2\"].append(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"question\": [], \"premise\": [], \"choice1\": [], \"choice2\": [], \"idx\": []}\n",
    "\n",
    "for key in [\"label\", \"question\", \"premise\", \"choice1\", \"choice2\", \"idx\"]:\n",
    "    for i in range(len(sentences[\"choice1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56251ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df451552",
   "metadata": {},
   "source": [
    "### sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb21ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/superglue_copa/temp-0.2_topp-1.0_maxt-2048/combined_True/sv_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT DE\n",
    "\n",
    "alternative_names = [\"Förutsättning\"]\n",
    "\n",
    "sentences = {\"premise\": [], \"choice1\": [], \"choice2\": []}\n",
    "\n",
    "for i, datapoint in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 3:\n",
    "        print(\"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint)\n",
    "        \n",
    "    elif len(datapoint.split(\"\\n\")) == 3:\n",
    "        \n",
    "        if \"\\n\\n\" in datapoint:\n",
    "            datapoint = datapoint.replace(\"\\n\\n\", \"\\n\")\n",
    "            \n",
    "        for elem in alternative_names:\n",
    "            if elem in datapoint:\n",
    "                datapoint = datapoint.replace(elem, \"Premiss\")\n",
    "                \n",
    "        sentence1, sentence2, sentence3 = datapoint.split(\"\\n\")\n",
    "        \n",
    "        if not \"Premiss\" in sentence1:\n",
    "            print(\"NO PREMISE\", sentence1)\n",
    "        if not \"Alternativ 1\" in sentence2:\n",
    "            print(\"Alternativ 1\", sentence2)\n",
    "        if not \"Alternativ 2\" in sentence3:\n",
    "            print(\"NO ALTERNATIVE 2\", sentence3)\n",
    "            \n",
    "    if not \"Premiss\" in sentence1:\n",
    "            sentence1 = \"Premiss: \" + \"\\\"\" + sentence1 + \"\\\"\"\n",
    "            print(\"no premise\", sentence1)\n",
    "    if not \"Alternativ 1\" in sentence2:\n",
    "            sentence2 = \"Alternativ 1: \" + \"\\\"\" + sentence2 + \"\\\"\"\n",
    "            print(\"no alternative 1\", sentence2)\n",
    "    if not \"Alternativ 2\" in sentence3:\n",
    "            sentence3 = \"Alternativ 2: \"+ \"\\\"\" + sentence3 + \"\\\"\"\n",
    "            print(\"no alternative 2\", sentence3)\n",
    "    \n",
    "    s1 = sentence1.split(\"Premiss\")[1].strip(\" :\")\n",
    "    s2 = sentence2.split(\"Alternativ 1\")[1].strip(\" :\")\n",
    "    s3 = sentence3.split(\"Alternativ 2\")[1].strip(\" :\")\n",
    "    \n",
    "    if s1.strip(\"\\\"\") != \"\": \n",
    "        s1 = cut_double_quotes(s1)\n",
    "    if s2.strip(\"\\\"\") != \"\": \n",
    "        s2 = cut_double_quotes(s2)\n",
    "    if s3.strip(\"\\\"\") != \"\": \n",
    "        s3 = cut_double_quotes(s3)\n",
    "    \n",
    "    sentences[\"premise\"].append(s1)\n",
    "    sentences[\"choice1\"].append(s2)\n",
    "    sentences[\"choice2\"].append(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"question\": [], \"premise\": [], \"choice1\": [], \"choice2\": [], \"idx\": []}\n",
    "\n",
    "for key in [\"label\", \"question\", \"premise\", \"choice1\", \"choice2\", \"idx\"]:\n",
    "    for i in range(len(sentences[\"choice1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a7139",
   "metadata": {},
   "source": [
    "# Belebele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe49844",
   "metadata": {},
   "source": [
    "### en paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21750ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"paraphrases/task/individual_belebele/temp-0.2_topp-1.0_maxt-2048/combined_True/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT DE\n",
    "\n",
    "sentences = {\"flores_passage\": [], \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "counter = 0\n",
    "\n",
    "for i, datapoint_orig in enumerate(data_orig[\"content\"]):\n",
    "            \n",
    "    datapoint = datapoint_orig.replace(\"\\n\\n\", \"\\n\").strip(\"\\n\")\n",
    "    datapoint = datapoint.replace(\":\\n\", \": \").replace(\": \\n\", \": \")\n",
    "    datapoint = datapoint.replace(\"Paraphrased\", \"\").replace(\"paraphrased\", \"\")\n",
    "    datapoint = datapoint.replace(\"Paraphrase\", \"\").replace(\"paraphrase\", \"\")\n",
    "    if \": Option A\" in datapoint:\n",
    "        datapoint = datapoint.replace(\": Option A\", \":\\nOption A\")\n",
    "    datapoint = datapoint.replace(\"Option A :\\nOption A:\", \"Option A:\")\n",
    "    datapoint = datapoint.replace(\"Option B :\\nOption B:\", \"Option B:\")\n",
    "    datapoint = datapoint.replace(\"Option C :\\nOption C:\", \"Option C:\")\n",
    "    datapoint = datapoint.replace(\"Option D :\\nOption D:\", \"Option D:\")\n",
    "    datapoint = datapoint.replace(\"answer options:\\n\", \"\").replace(\"Multiple-Choice Options :\\n\", \"\")\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) == 1:\n",
    "        counter += 1\n",
    "        passage = datapoint\n",
    "        question1, option1, option2, option3, option4 = \"\", \"\", \"\", \"\", \"\"\n",
    "    elif len(datapoint.split(\"\\n\")) != 6:\n",
    "        print(i, \"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint_orig)\n",
    "        if sum([\"Option A\" in elem for elem in datapoint.split(\"\\n\")]) == 2:\n",
    "            passage, question, option1, option2, option3, option4, _, _, _, _ = datapoint.split(\"\\n\")\n",
    "        elif len(datapoint.split(\"\\n\")) == 7: \n",
    "            _, passage, question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "        else:\n",
    "            print(\"\\n\\nPASSAGE MISSING\", len(datapoint.split(\"\\n\")))\n",
    "            counter +=1\n",
    "            passage = \"\"\n",
    "            question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "    else:                       \n",
    "        passage, question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "    \n",
    "    if \")\" in option1 or \"A.\" in option1:\n",
    "        option1 = option1.replace(\")\", \":\").replace(\"A.\", \"A:\")\n",
    "        option2 = option2.replace(\")\", \":\").replace(\"B.\", \"B:\")\n",
    "        option3 = option3.replace(\")\", \":\").replace(\"C.\", \"C:\")\n",
    "        option4 = option4.replace(\")\", \":\").replace(\"D.\", \"D:\")\n",
    "    \n",
    "    passage = passage.strip(\"\\n\")\n",
    "    question = question.strip(\"\\n\")\n",
    "    if passage != \"\":\n",
    "        passage = cut_double_quotes(passage)\n",
    "    if question != \"\":\n",
    "        question = cut_double_quotes(question)\n",
    "    if option1 != \"\":\n",
    "        option1 = cut_double_quotes(option1.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option2 != \"\":\n",
    "        option2 = cut_double_quotes(option2.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option3 != \"\":\n",
    "        option3 = cut_double_quotes(option3.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option4 != \"\":\n",
    "        option4 = cut_double_quotes(option4.split(\": \")[1].strip(\"\\n\"))\n",
    "        \n",
    "    sentences[\"flores_passage\"].append(passage)\n",
    "    sentences[\"question\"].append(question)\n",
    "    sentences[\"mc_answer1\"].append(option1)\n",
    "    sentences[\"mc_answer2\"].append(option2)\n",
    "    sentences[\"mc_answer3\"].append(option3)\n",
    "    sentences[\"mc_answer4\"].append(option4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50ccca",
   "metadata": {},
   "source": [
    "### de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_belebele/temp-0.2_topp-1.0_maxt-2048/combined_True/de_from_en/\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8416967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT DE\n",
    "\n",
    "sentences = {\"flores_passage\": [], \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "for i, datapoint_orig in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    datapoint = datapoint_orig.replace(\"\\n\\n\", \"\\n\").strip(\"\\n\")\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 6:\n",
    "        print(\"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint_orig)\n",
    "        \n",
    "        if len(datapoint.split(\"\\n\")) == 5:\n",
    "            passage = \"\"\n",
    "            question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "            \n",
    "        elif len(datapoint.split(\"\\n\")) == 1:\n",
    "            passage, question, option1, option2, option3, option4 = \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "        \n",
    "        else:\n",
    "            print(\"\\nsolution must be found\\n\")\n",
    "            \n",
    "    else:                       \n",
    "        passage, question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "    \n",
    "    passage = passage.strip(\"\\n\")\n",
    "    question = question.strip(\"\\n\")\n",
    "    if passage != \"\":\n",
    "        passage = cut_double_quotes(passage)\n",
    "    if question != \"\":\n",
    "        question = cut_double_quotes(question)\n",
    "    if option1 != \"\":\n",
    "        option1 = cut_double_quotes(option1.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option2 != \"\":\n",
    "        option2 = cut_double_quotes(option2.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option3 != \"\":\n",
    "        option3 = cut_double_quotes(option3.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option4 != \"\":\n",
    "        option4 = cut_double_quotes(option4.split(\": \")[1].strip(\"\\n\"))\n",
    "        \n",
    "    sentences[\"flores_passage\"].append(passage)\n",
    "    sentences[\"question\"].append(question)\n",
    "    sentences[\"mc_answer1\"].append(option1)\n",
    "    sentences[\"mc_answer2\"].append(option2)\n",
    "    sentences[\"mc_answer3\"].append(option3)\n",
    "    sentences[\"mc_answer4\"].append(option4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3878895",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"link\": [], \"question_number\": [], \"dialect\": [], \"ds\": [], \"flores_passage\": [], \n",
    "                 \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "for key in [\"label\", \"link\", \"question_number\", \"dialect\", \"ds\", \"flores_passage\", \"question\", \n",
    "            \"mc_answer1\", \"mc_answer2\", \"mc_answer3\", \"mc_answer4\"]:\n",
    "    for i in range(len(sentences[\"mc_answer1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615cb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 passages missing = 1.2%\n",
    "# for i in range(900):\n",
    "#     if data_combined[\"flores_passage\"][i] == \"\":\n",
    "#         print(i, data_combined[\"flores_passage\"][i], data_combined[\"question\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5c8d9",
   "metadata": {},
   "source": [
    "### nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2339a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_belebele/temp-0.2_topp-1.0_maxt-2048/combined_True/nl_from_en/\"\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT DE\n",
    "\n",
    "sentences = {\"flores_passage\": [], \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "counter = 0\n",
    "for i, datapoint_orig in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    datapoint = datapoint_orig.replace(\"\\n\\n\", \"\\n\").strip(\"\\n\")\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 6:\n",
    "        counter += 1\n",
    "        print(\"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint_orig, \"\\n\")\n",
    "        \n",
    "        if len(datapoint.split(\"\\n\")) == 5:\n",
    "            passage = \"\"\n",
    "            question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "            \n",
    "        elif len(datapoint.split(\"\\n\")) == 1:\n",
    "            passage, question, option1, option2, option3, option4 = \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "        \n",
    "        else:\n",
    "            print(\"\\nsolution must be found\\n\")\n",
    "            \n",
    "    else:                       \n",
    "        passage, question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "        \n",
    "    passage = passage.strip(\"\\n\")\n",
    "    question = question.strip(\"\\n\")\n",
    "    if passage != \"\":\n",
    "        passage = cut_double_quotes(passage)\n",
    "    if question != \"\":\n",
    "        question = cut_double_quotes(question)\n",
    "    if option1 != \"\":\n",
    "        option1 = cut_double_quotes(option1.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option2 != \"\":\n",
    "        option2 = cut_double_quotes(option2.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option3 != \"\":\n",
    "        option3 = cut_double_quotes(option3.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option4 != \"\":\n",
    "        option4 = cut_double_quotes(option4.split(\": \")[1].strip(\"\\n\"))\n",
    "        \n",
    "    sentences[\"flores_passage\"].append(passage)\n",
    "    sentences[\"question\"].append(question)\n",
    "    sentences[\"mc_answer1\"].append(option1)\n",
    "    sentences[\"mc_answer2\"].append(option2)\n",
    "    sentences[\"mc_answer3\"].append(option3)\n",
    "    sentences[\"mc_answer4\"].append(option4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"sentence_translations.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "data_combined = {\"label\": [], \"link\": [], \"question_number\": [], \"dialect\": [], \"ds\": [], \"flores_passage\": [], \n",
    "                 \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "for key in [\"label\", \"link\", \"question_number\", \"dialect\", \"ds\", \"flores_passage\", \"question\", \n",
    "            \"mc_answer1\", \"mc_answer2\", \"mc_answer3\", \"mc_answer4\"]:\n",
    "    for i in range(len(sentences[\"mc_answer1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "\n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ab48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21 passages missing = 2.3%\n",
    "# for i in range(900):\n",
    "#     if data_combined[\"flores_passage\"][i] == \"\":\n",
    "#         print(i, data_combined[\"flores_passage\"][i], data_combined[\"question\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6427276",
   "metadata": {},
   "source": [
    "### sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_belebele/temp-0.2_topp-1.0_maxt-2048/combined_True/sv_from_en/\"\n",
    "\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84611b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT SV\n",
    "\n",
    "sentences = {\"flores_passage\": [], \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "counter = 0\n",
    "for i, datapoint_orig in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    datapoint = datapoint_orig.replace(\"\\n\\n\", \"\\n\").strip(\"\\n\")\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 6:\n",
    "        counter += 1\n",
    "        print(\"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint_orig, \"\\n\")\n",
    "        \n",
    "        if len(datapoint.split(\"\\n\")) == 5:\n",
    "            passage = \"\"\n",
    "            question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "            \n",
    "        elif len(datapoint.split(\"\\n\")) == 1:\n",
    "            passage, question, option1, option2, option3, option4 = \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "            \n",
    "        else:\n",
    "            print(i, \"\\n!!!!!!!!!!!!!!!solution must be found!!!!!!!!!!!!!!!!!!\\n\")\n",
    "            \n",
    "    else:                       \n",
    "        passage, question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "    \n",
    "    passage = passage.strip(\"\\n\")\n",
    "    question = question.strip(\"\\n\")\n",
    "    if passage != \"\":\n",
    "        passage = cut_double_quotes(passage)\n",
    "    if question != \"\":\n",
    "        question = cut_double_quotes(question)\n",
    "    if option1 != \"\" and \": \" in option1:\n",
    "        option1 = cut_double_quotes(option1.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option2 != \"\" and \": \" in option2:\n",
    "        option2 = cut_double_quotes(option2.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option3 != \"\" and \": \" in option3:\n",
    "        option3 = cut_double_quotes(option3.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option4 != \"\" and \": \" in option4:\n",
    "        option4 = cut_double_quotes(option4.split(\": \")[1].strip(\"\\n\"))\n",
    "        \n",
    "    sentences[\"flores_passage\"].append(passage)\n",
    "    sentences[\"question\"].append(question)\n",
    "    sentences[\"mc_answer1\"].append(option1)\n",
    "    sentences[\"mc_answer2\"].append(option2)\n",
    "    sentences[\"mc_answer3\"].append(option3)\n",
    "    sentences[\"mc_answer4\"].append(option4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a26405",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = {\"label\": [], \"link\": [], \"question_number\": [], \"dialect\": [], \"ds\": [], \"flores_passage\": [], \n",
    "                 \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "for key in [\"label\", \"link\", \"question_number\", \"dialect\", \"ds\", \"flores_passage\", \"question\", \n",
    "            \"mc_answer1\", \"mc_answer2\", \"mc_answer3\", \"mc_answer4\"]:\n",
    "    for i in range(len(sentences[\"mc_answer1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "\n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25 passages missing = 2.8%\n",
    "#for i in range(900):\n",
    "#    if data_combined[\"flores_passage\"][i] == \"\":\n",
    "#        print(i, data_combined[\"flores_passage\"][i], data_combined[\"question\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f099e",
   "metadata": {},
   "source": [
    "### it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"translations/task/individual_belebele/temp-0.2_topp-1.0_maxt-2048/combined_True/it_from_en/\"\n",
    "with open(path + \"dataset.pkl\", \"rb\") as f:\n",
    "    data_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4baf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize PAWS-X CONTENT SV\n",
    "\n",
    "sentences = {\"flores_passage\": [], \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "counter = 0\n",
    "for i, datapoint_orig in enumerate(data_orig[\"content\"]):\n",
    "    \n",
    "    datapoint = datapoint_orig.replace(\"\\n\\n\", \"\\n\").strip(\"\\n\")\n",
    "    \n",
    "    if len(datapoint.split(\"\\n\")) != 6:\n",
    "        counter += 1\n",
    "        print(i, \"NOT THE RIGHT NUMBER OF SENTENCES\", datapoint_orig, \"\\n\")\n",
    "        \n",
    "        if len(datapoint.split(\"\\n\")) == 5:\n",
    "            passage = \"\"\n",
    "            question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "            \n",
    "        else:\n",
    "            print(i, \"SET TO NONE\")\n",
    "            passage, question, option1, option2, option3, option4 = \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "    else:                       \n",
    "        passage, question, option1, option2, option3, option4 = datapoint.split(\"\\n\")\n",
    "    \n",
    "    passage = passage.strip(\"\\n\")\n",
    "    question = question.strip(\"\\n\")\n",
    "    if passage != \"\":\n",
    "        passage = cut_double_quotes(passage)\n",
    "    if question != \"\":\n",
    "        question = cut_double_quotes(question)\n",
    "    if option1 != \"\" and \": \" in option1:\n",
    "        option1 = cut_double_quotes(option1.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option2 != \"\" and \": \" in option2:\n",
    "        option2 = cut_double_quotes(option2.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option3 != \"\" and \": \" in option3:\n",
    "        option3 = cut_double_quotes(option3.split(\": \")[1].strip(\"\\n\"))\n",
    "    if option4 != \"\" and \": \" in option4:\n",
    "        option4 = cut_double_quotes(option4.split(\": \")[1].strip(\"\\n\"))\n",
    "        \n",
    "    sentences[\"flores_passage\"].append(passage)\n",
    "    sentences[\"question\"].append(question)\n",
    "    sentences[\"mc_answer1\"].append(option1)\n",
    "    sentences[\"mc_answer2\"].append(option2)\n",
    "    sentences[\"mc_answer3\"].append(option3)\n",
    "    sentences[\"mc_answer4\"].append(option4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc277bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = {\"label\": [], \"link\": [], \"question_number\": [], \"dialect\": [], \"ds\": [], \"flores_passage\": [], \n",
    "                 \"question\": [], \"mc_answer1\": [], \"mc_answer2\": [], \"mc_answer3\": [], \"mc_answer4\": []}\n",
    "\n",
    "for key in [\"label\", \"link\", \"question_number\", \"dialect\", \"ds\", \"flores_passage\", \"question\", \n",
    "            \"mc_answer1\", \"mc_answer2\", \"mc_answer3\", \"mc_answer4\"]:\n",
    "    for i in range(len(sentences[\"mc_answer1\"])):\n",
    "        if key in sentences.keys():\n",
    "            data_combined[key].append(sentences[key][i])\n",
    "        else:\n",
    "            data_combined[key].append(data_orig[key][i])\n",
    "            \n",
    "data_combined = Dataset.from_dict(data_combined)\n",
    "with open(path + \"dataset_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 passages missing = 1.3%\n",
    "# for i in range(900):\n",
    "#     if data_combined[\"flores_passage\"][i] == \"\":\n",
    "#         print(i, data_combined[\"flores_passage\"][i], data_combined[\"question\"][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
